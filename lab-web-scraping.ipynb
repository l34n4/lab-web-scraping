{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786",
      "metadata": {
        "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786"
      },
      "source": [
        "# Lab | Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8d9ee3d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Number     Name                                            Summary Guests\n",
            "0      1  \"Pilot\"  After an unsuccessful visit to the high-IQ spe...       \n"
          ]
        }
      ],
      "source": [
        "#try \n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the Wikipedia page for The Big Bang Theory Season 1\n",
        "url = 'https://en.wikipedia.org/wiki/The_Big_Bang_Theory_(season_1)'\n",
        "\n",
        "# Send an HTTP request to the page and get the HTML content\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Initialize lists to store the data\n",
        "episode_numbers = []\n",
        "episode_names = []\n",
        "episode_summaries = []\n",
        "episode_guests = []\n",
        "\n",
        "# Search for elements directly containing episode information\n",
        "episode_elements = soup.find_all(\"table\", {\"class\":\"wikitable plainrowheaders wikiepisodetable\"})\n",
        "\n",
        "for episode_element in episode_elements:\n",
        "    # Episode number\n",
        "    number = episode_element.find(\"td\", {\"style\":\"text-align:center\"}).text.strip()\n",
        "    episode_numbers.append(number)\n",
        "\n",
        "    # Title\n",
        "    name = episode_element.find('td', {'class': 'summary'}).text.strip().replace(\"'\", '\"')\n",
        "    episode_names.append(name)\n",
        "\n",
        "    # Extract the episode summary\n",
        "    summary = episode_element.find(\"td\", {'class': 'description'}).text.strip().replace(\"'\", '\"')\n",
        "    episode_summaries.append(summary)\n",
        "\n",
        "    # Guests when available\n",
        "    guests_element = episode_element.find('div', {'class': 'guest-stars'})\n",
        "    guests = guests_element.text.strip().replace(\"'\", '\"') if guests_element else ''\n",
        "    episode_guests.append(guests)\n",
        "\n",
        "# Create a DataFrame with the extracted data\n",
        "data = {\n",
        "    'Number': episode_numbers,\n",
        "    'Name': episode_names,\n",
        "    'Summary': episode_summaries,\n",
        "    'Guests': episode_guests\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d9b9e7f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Numéro                                                Nom         directed\n",
            "0       1                                              Pilot  Mark Cendrowski\n",
            "1       2                            The Big Bran Hypothesis  Mark Cendrowski\n",
            "2       3                          The Fuzzy Boots Corollary  Mark Cendrowski\n",
            "3       4                           The Luminous Fish Effect  Mark Cendrowski\n",
            "4       5                            The Hamburger Postulate  Mark Cendrowski\n",
            "5       6                          The Middle-earth Paradigm  Mark Cendrowski\n",
            "6       7                               The Dumpling Paradox  Mark Cendrowski\n",
            "7       8                         The Grasshopper Experiment  Mark Cendrowski\n",
            "8       9                 The Cooper-Hofstadter Polarization  Mark Cendrowski\n",
            "9      10                               The Loobenfeld Decay  Mark Cendrowski\n",
            "10     11                         The Pancake Batter Anomaly  Mark Cendrowski\n",
            "11     12                              The Jerusalem Duality  Mark Cendrowski\n",
            "12     13                             The Bat Jar Conjecture  Mark Cendrowski\n",
            "13     14                          The Nerdvana Annihilation  Mark Cendrowski\n",
            "14     15  The Shiksa Indeterminacy[24]The Pork Chop Inde...  Mark Cendrowski\n",
            "15     16                                The Peanut Reaction  Mark Cendrowski\n",
            "16     17                               The Tangerine Factor  Mark Cendrowski\n"
          ]
        }
      ],
      "source": [
        "#using column \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL de la page Wikipedia pour la saison 1 de The Big Bang Theory\n",
        "url = \"https://en.wikipedia.org/wiki/The_Big_Bang_Theory_(season_1)\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "table = soup.find(\"table\", {\"class\": \"wikitable plainrowheaders wikiepisodetable\"})\n",
        "\n",
        "# list variable\n",
        "episode_numbers = []\n",
        "episode_names = []\n",
        "directed = []\n",
        "\n",
        "# by knowing it s a table \n",
        "for row in table.find_all(\"tr\")[1:]:  #header\n",
        "    columns = row.find_all([\"th\", \"td\"])\n",
        "\n",
        "    if len(columns) >= 4:\n",
        "       \n",
        "        number = columns[1].text.strip()\n",
        "        episode_numbers.append(number)\n",
        "\n",
        "        name = columns[2].text.strip().replace('\"', \"\")\n",
        "        episode_names.append(name)\n",
        "\n",
        "        directed_by = columns[3].text.strip()\n",
        "        directed.append(guests)\n",
        "\n",
        "#  DataFrame\n",
        "data = {\n",
        "    'Numéro': episode_numbers,\n",
        "    'Nom': episode_names,\n",
        "    'directed': directed_by\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Afficher le DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d493367f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Summary\n",
            "0   After an unsuccessful visit to the high-IQ spe...\n",
            "1   When Sheldon and Leonard drop off a box of fla...\n",
            "2   When a dejected Leonard sees Penny kissing a m...\n",
            "3   At the university, Sheldon is fired from his j...\n",
            "4   The guys, eating at the Cheesecake Factory whe...\n",
            "5   Penny invites the guys to her Halloween party....\n",
            "6   Penny's promiscuous acquaintance Christy from ...\n",
            "7   Raj introduces his parents, Dr. V.M. Koothrapp...\n",
            "8   Leonard cleans up after an Internet experiment...\n",
            "9   Penny is given a part in the musical Rent, but...\n",
            "10  Penny returns from visiting family in Nebraska...\n",
            "11  Sheldon becomes envious when he meets 15-year-...\n",
            "12  The guys decide to compete in a university qui...\n",
            "13  In an online auction, Leonard buys a full-size...\n",
            "14  Sheldon introduces his twin sister Missy to th...\n",
            "15  Penny learns that Leonard has never had a birt...\n",
            "16  Sheldon, determined to prove that the Chinese ...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the Wikipedia page for The Big Bang Theory Season 1\n",
        "url = 'https://en.wikipedia.org/wiki/The_Big_Bang_Theory_(season_1)'\n",
        "\n",
        "# Send an HTTP request to the page and get the HTML content\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find the table containing episode information\n",
        "table = soup.find(\"table\", {\"class\": \"wikitable plainrowheaders wikiepisodetable\"})\n",
        "\n",
        "# Find all td elements with class 'description' within the table\n",
        "descriptions = table.find_all('td', {'class': 'description'})\n",
        "\n",
        "# Extract episode summaries from the descriptions\n",
        "episode_summaries = [description.text.strip() for description in descriptions]\n",
        "\n",
        "# Create a DataFrame with the extracted data\n",
        "data = {\n",
        "    'Summary': episode_summaries\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f72d553b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Numéro                                                Nom  \\\n",
            "0       1                                              Pilot   \n",
            "1       2                            The Big Bran Hypothesis   \n",
            "2       3                          The Fuzzy Boots Corollary   \n",
            "3       4                           The Luminous Fish Effect   \n",
            "4       5                            The Hamburger Postulate   \n",
            "5       6                          The Middle-earth Paradigm   \n",
            "6       7                               The Dumpling Paradox   \n",
            "7       8                         The Grasshopper Experiment   \n",
            "8       9                 The Cooper-Hofstadter Polarization   \n",
            "9      10                               The Loobenfeld Decay   \n",
            "10     11                         The Pancake Batter Anomaly   \n",
            "11     12                              The Jerusalem Duality   \n",
            "12     13                             The Bat Jar Conjecture   \n",
            "13     14                          The Nerdvana Annihilation   \n",
            "14     15  The Shiksa Indeterminacy[24]The Pork Chop Inde...   \n",
            "15     16                                The Peanut Reaction   \n",
            "16     17                               The Tangerine Factor   \n",
            "\n",
            "            directed                                            Summary  \n",
            "0      James Burrows  After an unsuccessful visit to the high-IQ spe...  \n",
            "1    Mark Cendrowski  When Sheldon and Leonard drop off a box of fla...  \n",
            "2    Mark Cendrowski  When a dejected Leonard sees Penny kissing a m...  \n",
            "3    Mark Cendrowski  At the university, Sheldon is fired from his j...  \n",
            "4   Andrew D. Weyman  The guys, eating at the Cheesecake Factory whe...  \n",
            "5    Mark Cendrowski  Penny invites the guys to her Halloween party....  \n",
            "6    Mark Cendrowski  Penny's promiscuous acquaintance Christy from ...  \n",
            "7           Ted Wass  Raj introduces his parents, Dr. V.M. Koothrapp...  \n",
            "8        Joel Murray  Leonard cleans up after an Internet experiment...  \n",
            "9    Mark Cendrowski  Penny is given a part in the musical Rent, but...  \n",
            "10   Mark Cendrowski  Penny returns from visiting family in Nebraska...  \n",
            "11   Mark Cendrowski  Sheldon becomes envious when he meets 15-year-...  \n",
            "12   Mark Cendrowski  The guys decide to compete in a university qui...  \n",
            "13   Mark Cendrowski  In an online auction, Leonard buys a full-size...  \n",
            "14   Mark Cendrowski  Sheldon introduces his twin sister Missy to th...  \n",
            "15   Mark Cendrowski  Penny learns that Leonard has never had a birt...  \n",
            "16   Mark Cendrowski  Sheldon, determined to prove that the Chinese ...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL de la page Wikipedia pour la saison 1 de The Big Bang Theory\n",
        "url = \"https://en.wikipedia.org/wiki/The_Big_Bang_Theory_(season_1)\"\n",
        "response = requests.get(url)\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "# Extracting information from the first table\n",
        "table1 = soup.find(\"table\", {\"class\": \"wikitable plainrowheaders wikiepisodetable\"})\n",
        "episode_numbers = []\n",
        "episode_names = []\n",
        "directed = []\n",
        "\n",
        "for row in table1.find_all(\"tr\")[1:]:\n",
        "    columns = row.find_all([\"th\", \"td\"])\n",
        "\n",
        "    if len(columns) >= 4:\n",
        "        number = columns[1].text.strip()\n",
        "        episode_numbers.append(number)\n",
        "\n",
        "        name = columns[2].text.strip().replace('\"', \"\")\n",
        "        episode_names.append(name)\n",
        "\n",
        "        directed_by = columns[3].text.strip()\n",
        "        directed.append(directed_by)\n",
        "\n",
        "# Creating the first DataFrame\n",
        "data1 = {\n",
        "    'Numéro': episode_numbers,\n",
        "    'Nom': episode_names,\n",
        "    'directed': directed\n",
        "}\n",
        "\n",
        "df1 = pd.DataFrame(data1)\n",
        "\n",
        "# Extracting information from the second table\n",
        "table2 = soup.find(\"table\", {\"class\": \"wikitable plainrowheaders wikiepisodetable\"})\n",
        "descriptions = table2.find_all('td', {'class': 'description'})\n",
        "episode_summaries = [description.text.strip() for description in descriptions]\n",
        "\n",
        "# Creating the second DataFrame\n",
        "data2 = {\n",
        "    'Summary': episode_summaries\n",
        "}\n",
        "\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "# Merging the two DataFrames on index\n",
        "result_df = pd.concat([df1, df2], axis=1)\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(result_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8882fc-4815-4567-92fa-b4816358ba7d",
      "metadata": {
        "id": "ce8882fc-4815-4567-92fa-b4816358ba7d"
      },
      "source": [
        "Welcome to the IMDb Web Scraping Adventure Lab!\n",
        "\n",
        "**Objective**\n",
        "\n",
        "In this lab, we will embark on a mission to unearth valuable insights from the vast sea of data available on IMDb, one of the largest online databases of movie, TV, and celebrity information. As budding data scientists and business analysts, you have been tasked to scrape a specific subset of data from IMDb to assist film production companies in understanding the landscape of highly-rated movies in a defined time period. Your insights will potentially influence the making of the next netflix movie!\n",
        "\n",
        "**Background**\n",
        "\n",
        "In a world where data has become the new currency, businesses are leveraging big data to make informed decisions that drive success and profitability. The entertainment industry, being no exception, utilizes data analytics to comprehend market trends, audience preferences, and the performance of films based on various parameters such as director, genre, stars involved, etc. IMDb stands as a goldmine of such data, offering intricate details of almost every movie ever made.\n",
        "\n",
        "**Task**\n",
        "\n",
        "Your task is to create a Python script using `BeautifulSoup` and `pandas` to scrape IMDb movie data based on user ratings and release dates. This script should be able to filter movies with ratings above a certain threshold and within a specified date range.\n",
        "\n",
        "**Expected Outcome**\n",
        "\n",
        "- A function named `scrape_imdb` that takes four parameters: `title_type`,`user_rating`, `start_date`, and `end_date`.\n",
        "- The function should return a DataFrame with the following columns:\n",
        "  - **Movie Nr**: The number representing the movie’s position in the list.\n",
        "  - **Title**: The title of the movie.\n",
        "  - **Year**: The year the movie was released.\n",
        "  - **Rating**: The IMDb rating of the movie.\n",
        "  - **Runtime (min)**: The duration of the movie in minutes.\n",
        "  - **Genre**: The genre of the movie.\n",
        "  - **Description**: A brief description of the movie.\n",
        "  - **Director**: The director of the movie.\n",
        "  - **Stars**: The main stars of the movie.\n",
        "  - **Votes**: The number of votes the movie received.\n",
        "  - **Gross ($M)**: The gross earnings of the movie in millions of USD.\n",
        "\n",
        "You will execute this script to scrape data for movies with the Title Type `Feature Film` that have a user rating of `7.5 and above` and were released between `January 1, 1990, and December 31, 1992`.\n",
        "\n",
        "Remember to experiment with different title types, dates and ratings to ensure your code is versatile and can handle various searches effectively!\n",
        "\n",
        "**Resources**\n",
        "\n",
        "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "- [IMDb Advanced Search](https://www.imdb.com/search/title/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80ede61",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87b5319",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27455e5a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3519921d-5890-445b-9a33-934ed8ee378c",
      "metadata": {
        "id": "3519921d-5890-445b-9a33-934ed8ee378c"
      },
      "source": [
        "**Hint**\n",
        "\n",
        "Your first mission is to familiarize yourself with the IMDb advanced search page. Head over to [IMDb advanced search](https://www.imdb.com/search/title/) and input the following parameters, keeping all other fields to their default values or blank:\n",
        "\n",
        "- **Title Type**: Feature film\n",
        "- **Release date**: From 1990 to 1992 (Note: You don't need to specify the day and month)\n",
        "- **User Rating**: 7.5 to -\n",
        "\n",
        "Upon searching, you'll land on a page showcasing a list of movies, each displaying vital details such as the title, release year, and crew information. Your task is to scrape this treasure trove of data.\n",
        "\n",
        "Carefully examine the resulting URL and construct your own URL to include all the necessary parameters for filtering the movies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a83a0d-a742-49f6-985e-e27887cbf922",
      "metadata": {
        "id": "25a83a0d-a742-49f6-985e-e27887cbf922"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Best of luck! Immerse yourself in the world of movies and may the data be with you!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0",
      "metadata": {
        "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0"
      },
      "source": [
        "**Important note**:\n",
        "\n",
        "In the fast-changing online world, websites often get updates and make changes. When you try this lab, the IMDb website might be different from what we expect.\n",
        "\n",
        "If you run into problems because of these changes, like new rules or things that stop you from getting data, don't worry! Instead, get creative.\n",
        "\n",
        "You can choose another website that interests you and is good for scraping data. Websites like Wikipedia or The New York Times are good options. The main goal is still the same: get useful data and learn how to scrape it from a website that you find interesting. It's a chance to practice your web scraping skills and explore a source of information you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "40359eee-9cd7-4884-bfa4-83344c222305",
      "metadata": {
        "id": "40359eee-9cd7-4884-bfa4-83344c222305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   403 Forbidden\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <center>\n",
            "   <h1>\n",
            "    403 Forbidden\n",
            "   </h1>\n",
            "  </center>\n",
            " </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your solution goes here\n",
        "# After lot of hours i finally discover that i was not allowed to scrap so i will try somewhere else  \n",
        "# 1 the one where i realize the problem\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "url = \"https://www.imdb.com/search/title/?title_type=feature&user_rating=7.5,10&release_date=1990-01-01,1992-12-31\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "movie_list = soup.find_all(\"li\", class_=\"ipc-metadata-list-summary-item\")\n",
        "   \n",
        "    # title\n",
        "titles = []\n",
        "for movie in soup.find_all('h3', {'class':'ipc-title__text'}):\n",
        "    title = movie.text.strip()\n",
        "    titles.append(title)\n",
        "\n",
        "print (titles)\n",
        "print(soup.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "66e0c408",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   403 Forbidden\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <center>\n",
            "   <h1>\n",
            "    403 Forbidden\n",
            "   </h1>\n",
            "  </center>\n",
            " </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "    'Referer': 'https://www.google.com/'}\n",
        "\n",
        "url = \"https://www.imdb.com/search/title/?title_type=feature&user_rating=7.5,10&release_date=1990-01-01,1992-12-31\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "movie_list = soup.find_all(\"li\", class_=\"ipc-metadata-list-summary-item\")\n",
        "   \n",
        "    # title\n",
        "titles = []\n",
        "for movie in soup.find_all('h3', {'class':'ipc-title__text'}):\n",
        "    title = movie.text.strip()\n",
        "    titles.append(title)\n",
        "\n",
        "print (titles)\n",
        "print(soup.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2918bcf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping completed.\n",
            "Empty DataFrame\n",
            "Columns: [Title, Year, Rating, Runtime (min), Description, Votes]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "#3 how i started \n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "url = \"https://www.imdb.com/search/title/?title_type=feature&user_rating=7.5,10&release_date=1990-01-01,1992-12-31\"\n",
        "\n",
        "def scrape_imdb(title_type, user_rating, start_date, end_date):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    movie_list = soup.find_all(\"li\", class_=\"ipc-metadata-list-summary-item\")\n",
        "\n",
        "    data = {\n",
        "        #'Movie Nr': [],\n",
        "        'Title': [],\n",
        "        'Year': [],\n",
        "        'Rating': [],\n",
        "        'Runtime (min)': [],\n",
        "        #'Genre': [],\n",
        "        'Description': [],\n",
        "        #'Director': [],\n",
        "        #'Stars': [],\n",
        "        'Votes': [],\n",
        "        #'Gross ($M)': []\n",
        "    }\n",
        "\n",
        "    for idx, movie in enumerate(movie_list, start=1):\n",
        "        title = movie.find(\"h3\", class_=\"ipc-title__text\").find('a').text\n",
        "        # more complicate year rating runtime \n",
        "        common_elements = movie.find_all('span', class_='sc-1e00898e-8 hsHAHC dli-title-metadata-item')\n",
        "        year = None\n",
        "        runtime = None\n",
        "        parental_control = None\n",
        "\n",
        "        for element in common_elements:\n",
        "            text = element.text.strip()\n",
        "            if 'h' and 'm' in text: \n",
        "                runtime = text \n",
        "            elif text.isdigit() and len(text) == 4:  \n",
        "                year = text\n",
        "            else:\n",
        "                parental_control = text\n",
        "    \n",
        "        # Append extracted information to respective lists\n",
        "        data['Year'].append(year)\n",
        "        data['Runtime (min)'].append(runtime)\n",
        "        data['Parental_Control'].append(parental_control)\n",
        "        \n",
        "        # didn't find a way for genre \n",
        "        description = movie.find_all('div', class_=\"ipc-html-content-inner-div\")[1].text.strip()\n",
        "        # didn't find a way for director \n",
        "        rating = movie.find('span', class_=\"sc-16b9dsl-3 sc-16b9dsl-5 gTLOHV ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb\")[0].text\n",
        "        votes = movie.find('span', class_=\"ipc-rating-star--voteCount\").text\n",
        "        # didn't find a way for gross \n",
        "\n",
        "        data['Movie Nr'].append(idx)\n",
        "        data['Title'].append(title)\n",
        "        data['Year'].append(year)\n",
        "        data['Rating'].append(rating)\n",
        "        data['Runtime (min)'].append(runtime)\n",
        "        # data['Genre'].append(genre)\n",
        "        data['Description'].append(description)\n",
        "        # data['Director'].append(director)\n",
        "        # data['Stars'].append(', '.join(stars))\n",
        "        data['Votes'].append(votes)\n",
        "        # data['Gross ($M)'].append(gross)\n",
        "\n",
        "    print(\"Scraping completed.\")\n",
        "    return data\n",
        "\n",
        "# Call the function and store the result in a variable\n",
        "result_data = scrape_imdb(title_type='feature', user_rating='7.5', start_date='1990-01-01', end_date='1992-12-31')\n",
        "\n",
        "# Create a DataFrame from the result_data\n",
        "result_df = pd.DataFrame(result_data)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(result_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ad29fbf2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch data. Status Code: 403\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_imdb_partial(title_type, user_rating, start_date, end_date):\n",
        "    # Construct IMDb advanced search URL\n",
        "    base_url = \"https://www.imdb.com/search/title/\"\n",
        "    url_params = f\"?title_type={title_type.lower()}&user_rating={user_rating},&release_date={start_date},{end_date}\"\n",
        "    search_url = base_url + url_params\n",
        "\n",
        "    # Send a request to IMDb and get the HTML content\n",
        "    response = requests.get(search_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract movie details from the HTML content\n",
        "        movie_details = []\n",
        "        for movie in soup.find_all('div', class_='lister-item mode-advanced'):\n",
        "            title = movie.h3.a.text.strip()\n",
        "            year = movie.h3.find('span', class_='lister-item-year').text.strip('()')\n",
        "            rating = float(movie.strong.text)\n",
        "            # Add more details as needed\n",
        "\n",
        "            movie_details.append([title, year, rating])\n",
        "\n",
        "        # Create a DataFrame from the collected movie details\n",
        "        columns = ['Title', 'Year', 'Rating']\n",
        "        df = pd.DataFrame(movie_details, columns=columns)\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        print(f\"Failed to fetch data. Status Code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "result_df_partial = scrape_imdb_partial(title_type='Feature Film', user_rating='7.5', start_date='1990-01-01', end_date='1992-12-31')\n",
        "\n",
        "if result_df_partial is not None:\n",
        "    print(result_df_partial)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "b480577c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Premier nom extrait: Menu\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "url = \"https://nameberry.com/userlist/view/224720\"\n",
        "def scrape_names(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    name_elements = soup.select(\"span\",attrs={\"class\":\"jsx-761758307 mr-10\"})\n",
        "\n",
        "\n",
        "    names = [name.text.strip() for name in name_elements]\n",
        "    return names\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    names = scrape_names(url)\n",
        "\n",
        "    if names:\n",
        "        # Print the first name if it exists\n",
        "        if len(names) > 0:\n",
        "            print(\"Premier nom extrait:\", names[0])\n",
        "        else:\n",
        "            print(\"Aucun nom extrait depuis le site web.\")\n",
        "    else:\n",
        "        print(\"Échec de l'extraction des noms depuis le site web.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "6e89b7c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aucun nom trouvé dans le conteneur spécifié.\n",
            "Échec de l'extraction des noms depuis le site web.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://nameberry.com/userlist/view/224720\"\n",
        "\n",
        "def scrape_names(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Use a more specific selector to target the container of name elements\n",
        "        name_container = soup.find('span', class_='jsx-761758307 mr-10')\n",
        "\n",
        "        # Check if the name container was found\n",
        "        if name_container:\n",
        "            # Extract individual name elements\n",
        "            name_elements = name_container.find_all('a', class_='user-list-name-link')\n",
        "            \n",
        "            # Check if any names were found\n",
        "            if name_elements:\n",
        "                names = [name.text.strip() for name in name_elements]\n",
        "                return names\n",
        "            else:\n",
        "                print(\"Aucun nom trouvé dans le conteneur spécifié.\")\n",
        "                return []\n",
        "        else:\n",
        "            print(\"Aucun conteneur de noms trouvé avec la classe spécifiée.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(f\"Échec de la requête HTTP. Statut de la réponse: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    names = scrape_names(url)\n",
        "\n",
        "    if names:\n",
        "        # Print the first name if it exists\n",
        "        if len(names) > 0:\n",
        "            print(\"Premier nom extrait:\", names[0])\n",
        "        else:\n",
        "            print(\"Aucun nom extrait depuis le site web.\")\n",
        "    else:\n",
        "        print(\"Échec de l'extraction des noms depuis le site web.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "ed0cc303",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/97/e3/fd7272d6d2c49fd49a79a603cb28c8b5a71f8911861b4a0409b3c006a241/selenium-4.17.2-py3-none-any.whl.metadata\n",
            "  Downloading selenium-4.17.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\leana\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/14/fb/9299cf74953f473a15accfdbe2c15218e766bae8c796f2567c83bae03e98/trio-0.24.0-py3-none-any.whl.metadata\n",
            "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\leana\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
            "Collecting typing_extensions>=4.9.0 (from selenium)\n",
            "  Obtaining dependency information for typing_extensions>=4.9.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\leana\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\leana\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in c:\\users\\leana\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\leana\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\leana\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\leana\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
            "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
            "Downloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
            "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 1.0/9.9 MB 20.5 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 2.8/9.9 MB 30.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 5.1/9.9 MB 36.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 7.4/9.9 MB 36.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.9/9.9 MB 42.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.9/9.9 MB 35.2 MB/s eta 0:00:00\n",
            "Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
            "   ---------------------------------------- 0.0/460.2 kB ? eta -:--:--\n",
            "   --------------------------------------- 460.2/460.2 kB 30.0 MB/s eta 0:00:00\n",
            "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: typing_extensions, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.2.0\n",
            "    Uninstalling sniffio-1.2.0:\n",
            "      Successfully uninstalled sniffio-1.2.0\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.17.2 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 typing_extensions-4.9.0 wsproto-1.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "bc7c9442",
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 193] %1 n’est pas une application Win32 valide",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[85], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     names \u001b[38;5;241m=\u001b[39m scrape_names(url)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m names:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# Print the first name if it exists\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[1;32mIn[85], line 20\u001b[0m, in \u001b[0;36mscrape_names\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m chrome_service \u001b[38;5;241m=\u001b[39m ChromeService(executable_path\u001b[38;5;241m=\u001b[39mchrome_path)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize the Chrome driver with options and service\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mchrome_service, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[0;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:98\u001b[0m, in \u001b[0;36mService.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Starts the Service.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    :Exceptions:\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m     - WebDriverException : Raised either when it can't start the service\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m       or when it can't connect to the service\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_process(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[0;32m    100\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:208\u001b[0m, in \u001b[0;36mService._start_process\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    205\u001b[0m         start_info\u001b[38;5;241m.\u001b[39mdwFlags \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mCREATE_NEW_CONSOLE \u001b[38;5;241m|\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSTARTF_USESHOWWINDOW\n\u001b[0;32m    206\u001b[0m         start_info\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    209\u001b[0m         cmd,\n\u001b[0;32m    210\u001b[0m         env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[0;32m    211\u001b[0m         close_fds\u001b[38;5;241m=\u001b[39mclose_file_descriptors,\n\u001b[0;32m    212\u001b[0m         stdout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_output,\n\u001b[0;32m    213\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_output,\n\u001b[0;32m    214\u001b[0m         stdin\u001b[38;5;241m=\u001b[39mPIPE,\n\u001b[0;32m    215\u001b[0m         creationflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_flags,\n\u001b[0;32m    216\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstart_info,\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopen_kw,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted executable: `\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m` in a child process with pid: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to output \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_output,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
            "File \u001b[1;32mc:\\Users\\leana\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 193] %1 n’est pas une application Win32 valide"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://nameberry.com/userlist/view/224720\"\n",
        "\n",
        "def scrape_names(url):\n",
        "    # Set the path to the ChromeDriver executable\n",
        "    chrome_path = \"C:\\\\Users\\\\leana\\\\Desktop\\\\chromedriver\"\n",
        "\n",
        "    # Set up Chrome options\n",
        "    chrome_options = ChromeOptions()\n",
        "    chrome_options.add_argument(\"--headless\")  # Optional: run Chrome in headless mode\n",
        "\n",
        "    # Set up the Chrome service\n",
        "    chrome_service = ChromeService(executable_path=chrome_path)\n",
        "\n",
        "    # Initialize the Chrome driver with options and service\n",
        "    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        # Wait for dynamic content to load (you might need to adjust the wait time)\n",
        "        driver.implicitly_wait(10)\n",
        "\n",
        "        # Get the page source after dynamic content has loaded\n",
        "        page_source = driver.page_source\n",
        "\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Use a more specific selector to target the container of name elements\n",
        "        name_container = soup.find('span', class_='jsx-761758307 mr-10')\n",
        "\n",
        "        # Check if the name container was found\n",
        "        if name_container:\n",
        "            # Extract individual name elements\n",
        "            name_elements = name_container.find_all('a', class_='user-list-name-link')\n",
        "            \n",
        "            # Check if any names were found\n",
        "            if name_elements:\n",
        "                names = [name.text.strip() for name in name_elements]\n",
        "                return names\n",
        "            else:\n",
        "                print(\"Aucun nom trouvé dans le conteneur spécifié.\")\n",
        "                return []\n",
        "        else:\n",
        "            print(\"Aucun conteneur de noms trouvé avec la classe spécifiée.\")\n",
        "            return []\n",
        "    finally:\n",
        "        # Close the web driver when done\n",
        "        driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    names = scrape_names(url)\n",
        "\n",
        "    if names:\n",
        "        # Print the first name if it exists\n",
        "        if len(names) > 0:\n",
        "            print(\"Premier nom extrait:\", names[0])\n",
        "        else:\n",
        "            print(\"Aucun nom extrait depuis le site web.\")\n",
        "    else:\n",
        "        print(\"Échec de l'extraction des noms depuis le site web.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "de6a6c4f",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "WebDriver.__init__() got an unexpected keyword argument 'executable_path'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[86], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Optional: run Chrome in headless mode\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the Chrome driver with options\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(executable_path\u001b[38;5;241m=\u001b[39mchrome_path, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Now you can use 'driver' to interact with the browser\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Example: navigate to a URL\u001b[39;00m\n\u001b[0;32m     17\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://nameberry.com/userlist/view/224720\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: WebDriver.__init__() got an unexpected keyword argument 'executable_path'"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Set the path to the ChromeDriver executable\n",
        "chrome_path = \"C:\\\\Users\\\\leana\\\\Desktop\\\\chromedriver\"\n",
        "\n",
        "# Set up Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: run Chrome in headless mode\n",
        "\n",
        "# Initialize the Chrome driver with options\n",
        "driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)\n",
        "\n",
        "# Now you can use 'driver' to interact with the browser\n",
        "\n",
        "# Example: navigate to a URL\n",
        "driver.get(\"https://nameberry.com/userlist/view/224720\")\n",
        "\n",
        "# Example: print the page title\n",
        "print(\"Page Title:\", driver.title)\n",
        "\n",
        "# Close the browser when done\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461411b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "url = \"https://momlovesbest.com/elf-names-for-baby#q0\"\n",
        "\n",
        "def scrape_names(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    name_elements = soup.select(\"h3\", attrs={\"class\": \"name-heading underline-main\"})\n",
        "\n",
        "    # Extract names without numbers using regular expressions\n",
        "    names = [re.sub(r'\\d+\\.\\s', '', name.text.strip()) for name in name_elements]\n",
        "\n",
        "    # Filter out non-name entries and entries with spaces in the middle\n",
        "    names = [name for name in names if any(c.isalpha() for c in name) and ' ' not in name]\n",
        "    return names\n",
        "\n",
        "# Retrieve names\n",
        "names = scrape_names(url)\n",
        "\n",
        "# Create a DataFrame with an index\n",
        "data = {'Elf Names': names}\n",
        "df = pd.DataFrame(data, index=range(1, len(names)+1))\n",
        "\n",
        "# Access the first name without the number using index 1\n",
        "print(\"Premier nom extrait:\", df['Elf Names'][1])\n",
        "print(\"\\nDataFrame:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab1217a-df9d-4a2b-a32c-2c5f1eecd52a",
      "metadata": {
        "id": "9ab1217a-df9d-4a2b-a32c-2c5f1eecd52a"
      },
      "source": [
        "## BONUS\n",
        "\n",
        "The search results span multiple pages, housing a total of 631 movies in our example with each page displaying 50 movies at most. To scrape data seamlessly from all pages, you'll need to dive deep into the structure of the URLs generated with each \"Next\" click.\n",
        "\n",
        "Take a close look at the following URLs:\n",
        "- First page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
        "  ```\n",
        "- Second page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
        "  ```\n",
        "- Third page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
        "  ```\n",
        "\n",
        "You should notice a pattern. There is a `start` parameter incrementing by 50 with each page, paired with a constant `ref_` parameter holding the value \"adv_nxt\".\n",
        "\n",
        "Modify your script so it's capable of iterating over all available pages to fetch data on all the 631 movies (631 is the total number of movies in the proposed example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483ba9ac",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236f01d4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ac4fc0-a12b-4a00-9266-2020166f0dea",
      "metadata": {
        "id": "21ac4fc0-a12b-4a00-9266-2020166f0dea"
      },
      "outputs": [],
      "source": [
        "# Your solution goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
